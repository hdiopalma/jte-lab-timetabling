# 15/4/2024


Hoenestly gw udah mager bikin jurnal, dan kayaknya gw lebih banyak ngeluh disini daripada ngomongin projekan. Kemaren udah ada banyak progress sih, walau ngga keliatan, soalnya lebih ke optimasi di background-nya, terutama bagian API yang ngurus penjadwalan. Sekarang udah ada schema config, jadi sebelum kita ngirim configurasi penjadwalan, nanti bakal divalidasi dulu. Disitu kita make library jsonschema, ini bentukannya:
```python
config_schema = {
    "type": "object",
    "properties": {
        "fitness": {
            "type": "object",
            "properties": {
                "group_assignment_conflict": {
                    "type": "object",
                    "properties": {
                        "max_threshold": {"type": "number"},
                        "conflict_penalty": {"type": "number"}
                    }
                },
                "assistant_distribution": {
                    "type": "object",
                    "properties": {
                        "max_group_threshold": {"type": "number"},
                        "max_shift_threshold": {"type": "number"},
                        "group_penalty": {"type": "number"},
                        "shift_penalty": {"type": "number"}
                    }
                },
            },
            "required": ["group_assignment_conflict", "assistant_distribution"]
        },
        "selection": {
            "type": "object",
            "properties": {
                "roulette_wheel": {"type": "boolean"},
                "tournament": {"type": "boolean"},
                "elitism": {"type": "boolean"},
                "tournament_size": {"type": "number"}
            },
            "required": ["roulette_wheel", "tournament", "elitism"]
        },
        "crossover": {
            "type": "object",
            "properties": {
                "single_point": {"type": "boolean"},
                "two_point": {"type": "boolean"},
                "uniform": {"type": "boolean"},
                "crossover_probability": {"type": "number"},
                "uniform_probability": {"type": "number"}
            },
            "required": ["single_point", "two_point", "uniform"]
        },
        "mutation": {
            "type": "object",
            "properties": {
                "swap": {"type": "boolean"},
                "shift": {"type": "boolean"},
                "random": {"type": "boolean"},
                "mutation_probability": {"type": "number"}
            },
            "required": ["swap", "shift", "random"]
        },
        "repair": {
            "type": "object",
            "properties": {
                "time_slot": {"type": "boolean"}
            },
            "required": ["time_slot"]
        },
        "neighborhood": {
            "type": "object",
            "properties": {
                "random_swap": {"type": "boolean"},
                "neighborhood_size": {"type": "number"}
            },
            "required": ["random_swap"]
        },
        "local_search": {
            "type": "object",
            "properties": {
                # "simulated_annealing": {"type": "boolean"},
                # "tabu_search": {"type": "boolean"},
                "algorithm": {
                    "type": "string",
                    "enum": ["simulated_annealing", "tabu_search"]
                },
                "simulated_annealing_config": {
                    "type": "object",
                    "properties": {
                        "initial_temperature": {"type": "number"},
                        "cooling_rate": {"type": "number"},
                        "max_iteration": {"type": "number"},
                        "max_time": {"type": "number"}
                    }
                },
                "tabu_search_config": {
                    "type": "object",
                    "properties": {
                        "tabu_list_size": {"type": "number"},
                        "max_iteration": {"type": "number"},
                        "max_time": {"type": "number"},
                        "max_iteration_without_improvement": {"type": "number"},
                        "max_time_without_improvement": {"type": "number"}
                    }
                }
            },
            "required": ["algorithm"]
        },
        "algorithm": {
            "type": "string",
            "enum": ["genetic_algorithm", "genetic_local_search"]
        },
        "max_iteration": {"type": "number"},
        "population_size": {"type": "number"},
        "elitism_size": {"type": "number"}
    },
    "required": ["fitness", "selection", "crossover", "mutation", "repair", "neighborhood", "local_search", "algorithm"]
}```

Gw juga sekalian bikin default value semisal ngga si front-end ngga lengkap ngasihnya:
```python
default_config = {
    "fitness": {
        "group_assignment_conflict": {
            "max_threshold": 3,
            "conflict_penalty": 1
        },
        "assistant_distribution": {
            "max_group_threshold": 15,
            "max_shift_threshold": 50,
            "group_penalty": 1,
            "shift_penalty": 1
        }
    },
    "selection": {
        "roulette_wheel": True,
        "tournament": True,
        "elitism": True,
        "tournament_size": 2
    },
    "crossover": {
        "single_point": True,
        "two_point": True,
        "uniform": True,
        "crossover_probability": 0.1,
        "uniform_probability": 0.5
    },
    "mutation": {
        "swap": True,
        "shift": True,
        "random": True,
        "mutation_probability": 0.1
    },
    "repair": {
        "time_slot": True
    },
    "neighborhood": {
        "random_swap": True,
        "neighborhood_size": 100
    },
    "local_search": {
        # "simulated_annealing": True,
        # "tabu_search": False,
        "algorithm": "simulated_annealing",
        "simulated_annealing_config": {
            "initial_temperature": 100,
            "cooling_rate": 0.1,
            "max_iteration": 1000,
            "max_time": 60
        },
        "tabu_search_config": {
            "tabu_list_size": 50,
            "max_iteration": 1000,
            "max_time": 60,
            "max_iteration_without_improvement": 100,
            "max_time_without_improvement": 5
        }
    },
    "algorithm": "genetic_local_search",
    "max_iteration": 500,
    "population_size": 25,
    "elitism_size": 2
}```

Jadi setidaknya udah bakal terjamin lengkap konfigurasinya, tinggal validasi value-nya aja, dan yang setelah gw tes keknya berfungsi deh, setidaknya satu kepusingan udah lewat. Ini kode buat generate dan load si configurasi-nya:
```python
class ScheduleConfiguration:
    def __init__(self, data):
        self.data = data
        self.schema = config_schema
        self.default = default_config

    def validate_config(self, data):
        try:
            jsonschema.validate(data, self.schema)
        except jsonschema.exceptions.ValidationError as e:
            return False, e.message
        return True, None
        
    def load_config(self, data):
        config = self.default.copy()
        config.update(data)
        # validate config
        print("Loading configuration...")
        is_valid, error_message = self.validate_config(config)
        if not is_valid:
            raise ValueError(f"Invalid configuration: {error_message}")
        self.data = config
        print("Configuration loaded successfully.")
        return config

    @classmethod
    def from_data(cls, data):
        '''Create instance from data'''
        instance = cls(data)
        instance.load_config(data)
        return instance
```

Jadi dia bakal copy si default value-nya, terus di replace datanya ama data yang dikirim.

Selain itu udah sempet bikin database juga buat nyimpen proses dan hasilnya, dibikin dua tabel, satu buat nyimpen informasi proses dan konfigurasinya, satulagi buat nyimpen jadwalnya deh. Dipisah karena rencananya pas generate jadwal pengen dibikin buat bekerja secara latarbelakang gitu, jadi si user ngga perlu nunggu lama2 amat. Pertama disimpen dulu informasi kalo jadwal sedang dibuat, ketika kelar disimpen deh kedalam database beserta id si proses itu. Ini kodenya:
```python
class Solution(models.Model):
    id = models.AutoField(primary_key=True)
    name = models.CharField(max_length=50)
    semester = models.ForeignKey(Semester, on_delete=models.CASCADE, related_name='solutions')
    start_time = models.DateTimeField(auto_now_add=True)
    end_time = models.DateTimeField(default=timezone.now)
    status = models.CharField(max_length=32, default="Pending")
    #iteration log that contains the fitness value for each iteration
    iteration_log = models.JSONField(default=list)
    # configuration
    fitness = models.JSONField(default=dict)
    selection = models.JSONField(default=dict)
    crossover = models.JSONField(default=dict)
    mutation = models.JSONField(default=dict)
    repair = models.JSONField(default=dict)
    neighborhood = models.JSONField(default=dict)
    algorithm = models.JSONField(default=dict)
    local_search = models.JSONField(default=dict)
    max_iteration = models.IntegerField(default=500)
    population_size = models.IntegerField(default=25)
    elitism_size = models.IntegerField(default=2)
    # solution
    best_fitness = models.FloatField(null=True, blank=True)
    time_elapsed = models.FloatField(null=True, blank=True)
    # best_solution = models.JSONField(null=True, blank=True)
    gene_count = models.IntegerField(null=True, blank=True)

    def save(self, *args, **kwargs):
        self.end_time = timezone.now()
        super(Solution, self).save(*args, **kwargs)
    
    def __str__(self) -> str:
        return f"Solution for {self.semester.name} - {self.name}"
    
class ScheduleData(models.Model):
    '''Model for storing the schedule solution, daily schedule for each semester. Probably will be filled with huge data.'''
    id = models.AutoField(primary_key=True)
    solution = models.ForeignKey(Solution, on_delete=models.CASCADE, related_name='schedule_data')
    date = models.DateTimeField() # Date of the schedule.
    day = models.CharField(max_length=10) # Monday, Tuesday, etc.
    shift = models.CharField(max_length=10) # Shift1, Shift2, etc.
    laboratory = models.ForeignKey(Laboratory, on_delete=models.CASCADE, related_name='schedules')
    module = models.ForeignKey(Module, on_delete=models.CASCADE, related_name='schedules')
    chapter = models.ForeignKey(Chapter, on_delete=models.CASCADE, related_name='schedules')
    group = models.ForeignKey(Group, on_delete=models.CASCADE, related_name='schedules')
    assistant = models.ForeignKey(Assistant, on_delete=models.CASCADE, related_name='schedules')
    # time_slot = models.JSONField(default=dict)

    class Meta:
        # unique_together = ['date', 'day', 'shift', 'laboratory', 'module', 'chapter', 'group', 'assistant']
        indexes = [
            models.Index(fields=['laboratory', 'module', 'chapter', 'group', 'assistant'])
        ]
```

Yang bagian tabel solution kayak rada panjang sih, tadinya pengen dibikin satu field aja buat konfigurasi, tapi mending dipisah aja deh biar lebih rapih aja. Sebenernya gw sempet khawatir buat bikin table jadwal yang nyimpen jadwal per sesi, awalnya sempet kepikiran dimana 1 row data itu udah mengandung seluruh jadwal yang digenerate. Tapi setelah dipikir2 nanti bakal ribet buat milah2 datanya, kayak pengen dikelompokin di hari tertentu, atau grup tertentu, otomatis harus load seluruh jadwalnya dong. Kalo dipisah gini setidaknya gampang nyaringnya. Mungkin ngeri aja sih database bakal cepet penuh, soalnya sekali generate jadwal bakal ngehasilin ribuan jadwal sekaligus, takut bakal lemot juga pas nyimpen data. Tapi ternyata teknologi lebih canggih dari ekspetasi gw sih, nyimpen ratusan data sekaligus dan satu2 ternyata ngga nyampe sedetik. Tapi masih bisa dibikin lebih efisien sih mestinya. Jadi akhirnya gw nemu cara biar kalo insert data banyak cukup manggil database sekali aja, make bulkcreate bawaan django.

```python
def create_schedule_data(self, solution: Solution, best_chromosome: Chromosome):
        schedule_data_list = []
        for gene in best_chromosome:
            schedule_data_list.append(ScheduleData(
                solution=solution,
                laboratory_id=gene['laboratory'],
                module_id=gene['module'],
                chapter_id=gene['chapter'],
                group_id=gene['group'],
                assistant_id=gene['assistant'],
                date=gene['time_slot'].date,
                day=gene['time_slot'].day,
                shift=gene['time_slot'].shift
            ))

        with transaction.atomic():
            ScheduleData.objects.bulk_create(schedule_data_list)
```

Jadi semua data yang pengen disimpen dibikin kedalam list dulu, terus disimpen deh pake bulk_create, dia bakal cuman manggil database sekali sebanyak apapun data yang dikirim. Disini kita juga make transaction, ini buat pengaman aja sih, jadi semisal kalo salah satu data gagal disimpen, seluruh data ngga akan disimpen sekalian, buat mastiin aja sih datanya ngga berantakan. Setidaknya beberapa kepusingan telah teratasi, mungkin nanti gw mesti mikirin gimana cara manggil data banyak sekaligus yang lebih efisien. Selanjutnya keknya gw mesti nerapin fungsi pemrosesan asinkron deh, kayak yang gw bilang tadi, biar si user ngga perlu nunggu loading lama2, soalnya sekali generate data tuh bisa makan waktu yang cukup lama, yakali si user mesti nunggu loading dulu. Rencananya mau make redis dan celery sih, tapi ternyata ngga terlalu support windows, jadi sekarang gw mau baca2 dulu tentang dramatiq. Sebenernya gw masih was2 sih karena ngga pernah bimbingan dosen lagi, pasti gw bakal bingung mau nyampein apa karena banyak banget ya hal2 yang udah gw kerjain, tapi setidaknya gw pengen bimbingan ketika hasilnya udah keliatan dulu, soalnya gw bingung juga kalo yang ditunjukin cuman kode doang. Semoga ngga ada kendala atau masalah ya, mungkin kalo gw nunjukin progress yang udah jauh bakal lebih nerima, mungkin gw persiapin alesan juga kenapa gw ngga pernah minta bimbingan . . .

Duh ternyata bingung juta nentuin opsinya mau pake yang mana, antara dramatiq dan huey, pilih yang lebih simpel dan gampang aja kali ya? Lagi pula skala aplikasi nya kecil, ngga terlalu perlu yang ribet2, tapi 22nya sama2 simplel.

My reference: https://www.untangled.dev/2020/07/01/huey-minimal-task-queue-django/

Tapi mesti install layanan lagi sih, redis, jadinya ada 3 layanan yang jalan dong, vue.js buat front-end, python buat backend, dan redis buat broker (apadah broker?). Gapapa deh dicoba dulu, biar gampang kedepannya juga. Kalo udah lancar, selanjutnya bikin halaman dan menu di front-end deh, terus bikin tampilan semacem kalender, terus optimasi sistem penjadwalannya deh. (Sekarang masih terlalu tersebar jadwalnya, pengennya dibikin terpusat gitu, maksudnya di satu jadwal di prioritasin buat di penuhin slotnya buat beberapa grup sekaligus. Berjam2 cuman riset2 ini doang bjir (dan muter2 rumah ngga jelas). Bismillah.

Harusnya gw catet library apa aja yang gw pake, buat requrements.txt, tapi jadiin masalah nanti aja deh. 


-----

Hmm, ribet juga ya, belum sesmpet ngurusin yang dibahas sebelumnya, baru install redis dan huey doang, tapi karena kode yang gw tulis rada ribet, jadi keknya mesti gw rombak dikit deh. Mau dibikin lebih efisien, biar tinggal kirim2 konfigurasi antar kelas aja.